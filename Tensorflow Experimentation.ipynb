{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d8695bb",
   "metadata": {},
   "source": [
    "# Current Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5a315b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import TextVectorization, BatchNormalization\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import os\n",
    "from twilio.rest import Client\n",
    "import csv\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e04c81e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\anaconda3\\envs\\DncycP\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "162/162 [==============================] - 1s 4ms/step - loss: 0.2214 - mae: 0.4253 - mse: 0.2214\n",
      "Epoch 2/20\n",
      "162/162 [==============================] - 1s 4ms/step - loss: 0.1807 - mae: 0.4190 - mse: 0.1807\n",
      "Epoch 3/20\n",
      "162/162 [==============================] - 1s 4ms/step - loss: 0.1791 - mae: 0.4192 - mse: 0.1791\n",
      "Epoch 4/20\n",
      "162/162 [==============================] - 1s 4ms/step - loss: 0.1770 - mae: 0.4172 - mse: 0.1770\n",
      "Epoch 5/20\n",
      "162/162 [==============================] - 1s 4ms/step - loss: 0.1625 - mae: 0.3905 - mse: 0.1625\n",
      "Epoch 6/20\n",
      "162/162 [==============================] - 1s 4ms/step - loss: 0.0876 - mae: 0.2492 - mse: 0.0876\n",
      "Epoch 7/20\n",
      "162/162 [==============================] - 1s 4ms/step - loss: 0.0587 - mae: 0.1935 - mse: 0.0587\n",
      "Epoch 8/20\n",
      "162/162 [==============================] - 1s 4ms/step - loss: 0.0430 - mae: 0.1604 - mse: 0.0430\n",
      "Epoch 9/20\n",
      "162/162 [==============================] - 1s 4ms/step - loss: 0.0350 - mae: 0.1420 - mse: 0.0350\n",
      "Epoch 10/20\n",
      "162/162 [==============================] - 1s 4ms/step - loss: 0.0300 - mae: 0.1275 - mse: 0.0300\n",
      "Epoch 11/20\n",
      "162/162 [==============================] - 1s 4ms/step - loss: 0.0263 - mae: 0.1179 - mse: 0.0263\n",
      "Epoch 12/20\n",
      "162/162 [==============================] - 1s 4ms/step - loss: 0.0227 - mae: 0.1079 - mse: 0.0227\n",
      "Epoch 13/20\n",
      "162/162 [==============================] - 1s 4ms/step - loss: 0.0220 - mae: 0.1048 - mse: 0.0220\n",
      "Epoch 14/20\n",
      "162/162 [==============================] - 1s 4ms/step - loss: 0.0193 - mae: 0.0972 - mse: 0.0193\n",
      "Epoch 15/20\n",
      "162/162 [==============================] - 1s 4ms/step - loss: 0.0184 - mae: 0.0933 - mse: 0.0184\n",
      "Epoch 16/20\n",
      "162/162 [==============================] - 1s 4ms/step - loss: 0.0166 - mae: 0.0888 - mse: 0.0166\n",
      "Epoch 17/20\n",
      "162/162 [==============================] - 1s 4ms/step - loss: 0.0160 - mae: 0.0851 - mse: 0.0160\n",
      "Epoch 18/20\n",
      "162/162 [==============================] - 1s 4ms/step - loss: 0.0151 - mae: 0.0829 - mse: 0.0151\n",
      "Epoch 19/20\n",
      "162/162 [==============================] - 1s 4ms/step - loss: 0.0140 - mae: 0.0799 - mse: 0.0140\n",
      "Epoch 20/20\n",
      "162/162 [==============================] - 1s 4ms/step - loss: 0.0131 - mae: 0.0777 - mse: 0.0131\n",
      "MAE: 0.07146828253474553, MSE: 0.020426804805398664, r2: 0.8834895531739935, data set: chrV.csv\n"
     ]
    }
   ],
   "source": [
    "def preprocess_data(csv_file):\n",
    "    data = pd.read_csv(csv_file)\n",
    "\n",
    "    X = data.drop('c0', axis=1)\n",
    "    Y = data['c0'].values\n",
    "\n",
    "    categorical_cols = ['Kmer']\n",
    "    encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "    X_categorical_encoded = encoder.fit_transform(X[categorical_cols])\n",
    "\n",
    "    numerical_cols = ['skew', 'ratio']\n",
    "    scaler_X = StandardScaler()\n",
    "    X_numerical_scaled = scaler_X.fit_transform(X[numerical_cols])\n",
    "    \n",
    "    joblib.dump(encoder, 'encoder.pkl')\n",
    "    joblib.dump(scaler_X, 'standard_scaler.pkl')\n",
    "\n",
    "    return X_categorical_encoded, X_numerical_scaled, Y\n",
    "\n",
    "def create_model(input_shapes):\n",
    "    input_cat = tf.keras.layers.Input(shape=(input_shapes[0],))\n",
    "    cat_branch = tf.keras.layers.Dense(128, activation='tanh')(input_cat)\n",
    "    cat_branch = tf.keras.layers.Dropout(0.2)(cat_branch)\n",
    "\n",
    "    input_num = tf.keras.layers.Input(shape=(input_shapes[1],))\n",
    "    num_branch = tf.keras.layers.Dense(128, activation='relu')(input_num)\n",
    "    num_branch = tf.keras.layers.Dropout(0.2)(num_branch)\n",
    "\n",
    "    merged = tf.keras.layers.concatenate([cat_branch, num_branch])\n",
    "\n",
    "    merged = tf.keras.layers.Dense(64, activation='relu')(merged)\n",
    "    merged = tf.keras.layers.Dropout(0.2)(merged)\n",
    "    merged = tf.keras.layers.Dense(32, activation='sigmoid')(merged)\n",
    "    merged = tf.keras.layers.Dropout(0.2)(merged)\n",
    "    output = tf.keras.layers.Dense(1, activation='linear')(merged)\n",
    "\n",
    "    model = tf.keras.Model(inputs=[input_cat, input_num], outputs=output)\n",
    "    return model\n",
    "\n",
    "def compile_model(model):\n",
    "    model.compile(optimizer='adam',loss='mse', metrics = ['mae', 'mse'])\n",
    "    return model\n",
    "\n",
    "def get_model_layers_info(model):\n",
    "    model_summary = []\n",
    "    for layer in model.layers:\n",
    "        layer_info = {\n",
    "            \"Layer_Name\": layer.name,\n",
    "            \"Layer_Type\": type(layer).__name__,\n",
    "            \"Number_of_Nodes\": layer.units if hasattr(layer, 'units') else None,\n",
    "            \"Activation_Function\": layer.activation.__name__ if hasattr(layer, 'activation') else None\n",
    "        }\n",
    "        model_summary.append(layer_info)\n",
    "    return model_summary\n",
    "    \n",
    "def write_model_summary_to_csv(model, model_summary, output_file, X_cat_test, X_num_test, Y_test, data_set):\n",
    "    predictions = model.predict([X_cat_test, X_num_test])\n",
    "    true_targets = Y_test\n",
    "    predictions = np.squeeze(predictions)\n",
    "    true_targets = np.squeeze(true_targets)\n",
    "    mae = mean_absolute_error(true_targets, predictions)\n",
    "    mse = mean_squared_error(true_targets, predictions)\n",
    "    r2 = r2_score(true_targets, predictions)\n",
    "    print(f\"MAE: {mae}, MSE: {mse}, r2: {r2}, data set: {data_set}\")\n",
    "    \n",
    "    with open(output_file, mode='a', newline='') as file:\n",
    "        fieldnames = [\"Layer_Name\", \"Layer_Type\", \"Number_of_Nodes\", \"Activation_Function\"]\n",
    "        writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "        writer.writerow({\"Layer_Name\": \"MAE\", \"Layer_Type\": str(mae), \"Number_of_Nodes\": \"MSE\", \"Activation_Function\": str(mse)})\n",
    "        writer.writerow({\"Layer_Name\": \"R^2\", \"Layer_Type\": str(r2), \"Number_of_Nodes\": \"Data:\", \"Activation_Function\": data_set})\n",
    "        writer.writeheader()\n",
    "        for layer_info in model_summary:\n",
    "            writer.writerow(layer_info)\n",
    "\n",
    "# Load and preprocess data\n",
    "data_set = 'chrV.csv'\n",
    "X_cat, X_num, Y = preprocess_data(data_set)\n",
    "X_cat_train, X_cat_test, X_num_train, X_num_test, Y_train, Y_test = train_test_split(X_cat, X_num, Y, test_size=0.05, random_state=42)\n",
    "\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='mse', patience=10, restore_best_weights=True)\n",
    "model = create_model(input_shapes=(X_cat_train.shape[1], X_num_train.shape[1]))\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae', 'mse'])\n",
    "model.fit([X_cat_train, X_num_train], Y_train, epochs=20, batch_size=64, callbacks=[early_stopping])\n",
    "\n",
    "model_summary = get_model_layers_info(model)\n",
    "output_file = \"1_model_summary.csv\"\n",
    "write_model_summary_to_csv(model, model_summary, output_file, X_cat_test, X_num_test, Y_test, data_set)\n",
    "\n",
    "\n",
    "model.save('kmer_model.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db04161",
   "metadata": {},
   "source": [
    "## used for retraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94d74f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\anaconda3\\envs\\DncycP\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "162/162 [==============================] - 1s 4ms/step - loss: 0.1443 - mae: 0.3542 - mse: 0.1443\n",
      "Epoch 2/20\n",
      "162/162 [==============================] - 1s 4ms/step - loss: 0.1119 - mae: 0.3153 - mse: 0.1119\n",
      "Epoch 3/20\n",
      "162/162 [==============================] - 1s 4ms/step - loss: 0.0733 - mae: 0.2256 - mse: 0.0733\n",
      "Epoch 4/20\n",
      "162/162 [==============================] - 1s 4ms/step - loss: 0.0450 - mae: 0.1650 - mse: 0.0450\n",
      "Epoch 5/20\n",
      "162/162 [==============================] - 1s 4ms/step - loss: 0.0334 - mae: 0.1396 - mse: 0.0334\n",
      "Epoch 6/20\n",
      "162/162 [==============================] - 1s 4ms/step - loss: 0.0256 - mae: 0.1207 - mse: 0.0256\n",
      "Epoch 7/20\n",
      "162/162 [==============================] - 1s 3ms/step - loss: 0.0222 - mae: 0.1103 - mse: 0.0222\n",
      "Epoch 8/20\n",
      "162/162 [==============================] - 1s 3ms/step - loss: 0.0195 - mae: 0.1020 - mse: 0.0195\n",
      "Epoch 9/20\n",
      "162/162 [==============================] - 1s 4ms/step - loss: 0.0176 - mae: 0.0964 - mse: 0.0176\n",
      "Epoch 10/20\n",
      "162/162 [==============================] - 1s 4ms/step - loss: 0.0167 - mae: 0.0924 - mse: 0.0167\n",
      "Epoch 11/20\n",
      "162/162 [==============================] - 1s 3ms/step - loss: 0.0157 - mae: 0.0886 - mse: 0.0157\n",
      "Epoch 12/20\n",
      "162/162 [==============================] - 1s 3ms/step - loss: 0.0146 - mae: 0.0857 - mse: 0.0146\n",
      "Epoch 13/20\n",
      "162/162 [==============================] - 1s 4ms/step - loss: 0.0145 - mae: 0.0841 - mse: 0.0145\n",
      "Epoch 14/20\n",
      "162/162 [==============================] - 1s 4ms/step - loss: 0.0143 - mae: 0.0841 - mse: 0.0143\n",
      "Epoch 15/20\n",
      "162/162 [==============================] - 1s 4ms/step - loss: 0.0130 - mae: 0.0793 - mse: 0.0130\n",
      "Epoch 16/20\n",
      "162/162 [==============================] - 1s 4ms/step - loss: 0.0128 - mae: 0.0792 - mse: 0.0128\n",
      "Epoch 17/20\n",
      "162/162 [==============================] - 1s 4ms/step - loss: 0.0123 - mae: 0.0774 - mse: 0.0123\n",
      "Epoch 18/20\n",
      "162/162 [==============================] - 1s 4ms/step - loss: 0.0121 - mae: 0.0768 - mse: 0.0121\n",
      "Epoch 19/20\n",
      "162/162 [==============================] - 1s 4ms/step - loss: 0.0121 - mae: 0.0759 - mse: 0.0121\n",
      "Epoch 20/20\n",
      "162/162 [==============================] - 1s 4ms/step - loss: 0.0118 - mae: 0.0754 - mse: 0.0118\n",
      "MAE: 0.37005626701447436, MSE: 0.250283525984047, r2: -0.9467718804215572, data set: chrV.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "loaded_model = tf.keras.models.load_model('kmer_model.h5')\n",
    "\n",
    "# Load and preprocess the new dataset\n",
    "new_data_set = 'tiling.csv'\n",
    "X_cat, X_num, Y = preprocess_data(new_data_set)  # Make sure to define your preprocess_data function\n",
    "\n",
    "# Split new data into training and testing sets\n",
    "_cat_train, X_cat_test, X_num_train, X_num_test, Y_train, Y_test = train_test_split(X_cat, X_num, Y, test_size=0.05, random_state=42)\n",
    "\n",
    "# Define early stopping\n",
    "early_stopping = EarlyStopping(monitor='mse', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Continue training the loaded model with the new dataset\n",
    "loaded_model.compile(optimizer='adam', loss='mse', metrics=['mae', 'mse'])\n",
    "loaded_model.fit([X_cat_train, X_num_train], Y_train, epochs=20, batch_size=64, callbacks=[early_stopping])\n",
    "\n",
    "model_summary = get_model_layers_info(loaded_model)\n",
    "output_file = \"1_model_summary.csv\"\n",
    "write_model_summary_to_csv(loaded_model, model_summary, output_file, X_cat_test, X_num_test, Y_test, data_set)\n",
    "\n",
    "\n",
    "loaded_model.save('kmer_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "05e93863",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa4dc12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7acd3ebc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d9520b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3fa868",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e9bbe5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cdf3be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09a657a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.0243812   1.29044695 -0.0557848  -0.00295144  0.09518338]\n",
      " [-1.26117282  1.20498682  0.03296369 -0.15362114  0.07735078]\n",
      " [-0.94643374  1.20465036 -0.00581976 -0.11674812  0.07728254]\n",
      " ...\n",
      " [ 0.05762828 -0.24918125 -0.05563261 -0.00581419 -0.07353332]\n",
      " [ 0.07979301 -0.21923655 -0.05546259 -0.00556642 -0.07333217]\n",
      " [ 0.09087537 -0.25759268 -0.05535702 -0.00578457 -0.07356783]]\n",
      "Epoch 1/50\n",
      "677/677 [==============================] - 2s 2ms/step - loss: 0.4545 - mae: 0.3561 - mse: 0.1358\n",
      "Epoch 2/50\n",
      "677/677 [==============================] - 1s 2ms/step - loss: 0.1309 - mae: 0.3550 - mse: 0.1308\n",
      "Epoch 3/50\n",
      "677/677 [==============================] - 1s 2ms/step - loss: 0.1308 - mae: 0.3550 - mse: 0.1308\n",
      "Epoch 4/50\n",
      "677/677 [==============================] - 1s 2ms/step - loss: 0.1307 - mae: 0.3548 - mse: 0.1307\n",
      "Epoch 5/50\n",
      "677/677 [==============================] - 1s 2ms/step - loss: 0.1306 - mae: 0.3544 - mse: 0.1305\n",
      "Epoch 6/50\n",
      "677/677 [==============================] - 1s 2ms/step - loss: 0.1303 - mae: 0.3539 - mse: 0.1302\n",
      "Epoch 7/50\n",
      "677/677 [==============================] - 1s 2ms/step - loss: 0.1302 - mae: 0.3537 - mse: 0.1302\n",
      "Epoch 8/50\n",
      "677/677 [==============================] - 1s 2ms/step - loss: 0.1302 - mae: 0.3537 - mse: 0.1301\n",
      "Epoch 9/50\n",
      "677/677 [==============================] - 1s 2ms/step - loss: 0.1301 - mae: 0.3536 - mse: 0.1301\n",
      "Epoch 10/50\n",
      "677/677 [==============================] - 1s 2ms/step - loss: 0.1301 - mae: 0.3533 - mse: 0.1300\n",
      "Epoch 11/50\n",
      "677/677 [==============================] - 1s 2ms/step - loss: 0.1299 - mae: 0.3531 - mse: 0.1298\n",
      "Epoch 12/50\n",
      "677/677 [==============================] - 2s 2ms/step - loss: 0.1298 - mae: 0.3528 - mse: 0.1297\n",
      "Epoch 13/50\n",
      "677/677 [==============================] - 1s 2ms/step - loss: 0.1295 - mae: 0.3522 - mse: 0.1295\n",
      "Epoch 14/50\n",
      "677/677 [==============================] - 1s 2ms/step - loss: 0.1296 - mae: 0.3524 - mse: 0.1296\n",
      "Epoch 15/50\n",
      "677/677 [==============================] - 1s 2ms/step - loss: 0.1297 - mae: 0.3526 - mse: 0.1296\n",
      "Epoch 16/50\n",
      "677/677 [==============================] - 1s 2ms/step - loss: 0.1295 - mae: 0.3522 - mse: 0.1295\n",
      "Epoch 17/50\n",
      "677/677 [==============================] - 1s 2ms/step - loss: 0.1296 - mae: 0.3526 - mse: 0.1296\n",
      "Epoch 18/50\n",
      "677/677 [==============================] - 1s 2ms/step - loss: 0.1295 - mae: 0.3521 - mse: 0.1295\n",
      "Epoch 19/50\n",
      "677/677 [==============================] - 1s 2ms/step - loss: 0.1295 - mae: 0.3523 - mse: 0.1295\n",
      "Epoch 20/50\n",
      "677/677 [==============================] - 1s 2ms/step - loss: 0.1294 - mae: 0.3521 - mse: 0.1294\n",
      "Epoch 21/50\n",
      "677/677 [==============================] - 1s 2ms/step - loss: 0.1295 - mae: 0.3521 - mse: 0.1295\n",
      "Epoch 22/50\n",
      "677/677 [==============================] - 1s 2ms/step - loss: 0.1295 - mae: 0.3523 - mse: 0.1295\n",
      "Epoch 23/50\n",
      "677/677 [==============================] - 1s 2ms/step - loss: 0.1293 - mae: 0.3519 - mse: 0.1293\n",
      "Epoch 24/50\n",
      "677/677 [==============================] - 1s 2ms/step - loss: 0.1295 - mae: 0.3521 - mse: 0.1295\n",
      "Epoch 25/50\n",
      "677/677 [==============================] - 1s 2ms/step - loss: 0.1294 - mae: 0.3520 - mse: 0.1294\n",
      "Epoch 26/50\n",
      "677/677 [==============================] - 1s 2ms/step - loss: 0.1294 - mae: 0.3520 - mse: 0.1294\n",
      "Epoch 27/50\n",
      "677/677 [==============================] - 1s 2ms/step - loss: 0.1294 - mae: 0.3521 - mse: 0.1294\n",
      "Epoch 28/50\n",
      "677/677 [==============================] - 1s 2ms/step - loss: 0.1295 - mae: 0.3522 - mse: 0.1295\n",
      "Epoch 29/50\n",
      "677/677 [==============================] - 1s 2ms/step - loss: 0.1294 - mae: 0.3518 - mse: 0.1294\n",
      "Epoch 30/50\n",
      "677/677 [==============================] - 1s 2ms/step - loss: 0.1294 - mae: 0.3521 - mse: 0.1294\n",
      "Epoch 31/50\n",
      "677/677 [==============================] - 1s 2ms/step - loss: 0.1293 - mae: 0.3520 - mse: 0.1293\n",
      "Epoch 32/50\n",
      "677/677 [==============================] - 1s 2ms/step - loss: 0.1294 - mae: 0.3519 - mse: 0.1294\n",
      "Epoch 33/50\n",
      "677/677 [==============================] - 1s 2ms/step - loss: 0.1295 - mae: 0.3523 - mse: 0.1295\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.1213 - mae: 0.3406 - mse: 0.1213\n",
      "Test Loss: [0.12133223563432693, 0.3406246304512024, 0.12130477279424667]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_model_layers_info(model):\n",
    "    model_summary = []\n",
    "    for layer in model.layers:\n",
    "        layer_info = {\n",
    "            \"Layer_Name\": layer.name,\n",
    "            \"Layer_Type\": type(layer).__name__,\n",
    "            \"Number_of_Nodes\": layer.units if hasattr(layer, 'units') else None,\n",
    "            \"Activation_Function\": layer.activation.__name__ if hasattr(layer, 'activation') else None\n",
    "        }\n",
    "        model_summary.append(layer_info)\n",
    "    return model_summary\n",
    "\n",
    "\n",
    "def write_model_summary_to_csv(model_summary, output_file, X_tes, Y_tes, data_set):\n",
    "    predictions = model.predict([X_tes[:, :input_shape_dna], X_tes[:, input_shape_dna:]])\n",
    "    true_targets = Y_tes\n",
    "    predictions = np.squeeze(predictions)\n",
    "    true_targets = np.squeeze(true_targets)\n",
    "    mae = mean_absolute_error(true_targets, predictions)\n",
    "    mse = mean_squared_error(true_targets, predictions)\n",
    "    r2 = r2_score(true_targets, predictions)\n",
    "    \n",
    "    with open(output_file, mode='a', newline='') as file:\n",
    "        fieldnames = [\"Layer_Name\", \"Layer_Type\", \"Number_of_Nodes\", \"Activation_Function\"]\n",
    "        writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "                \n",
    "        writer.writerow({})  # Write an empty row\n",
    "        writer.writeheader()  # Write the header row\n",
    "        \n",
    "        writer.writerow({\"Layer_Name\": \"MAE\", \"Layer_Type\": str(mae), \"Number_of_Nodes\": \"MSE\", \"Activation_Function\": str(mse)})\n",
    "        writer.writerow({\"Layer_Name\": \"R^2\", \"Layer_Type\": str(r2), \"Number_of_Nodes\": \"Data:\", \"Activation_Function\": data_set})\n",
    "\n",
    "        \n",
    "        for layer_info in model_summary:\n",
    "            writer.writerow(layer_info)\n",
    "\n",
    "def custom_dna_one_hot_encoder(data_frame, column_name, sequence_length):\n",
    "    base_to_index = {'A': 1, 'C': 2, 'G': 3, 'T': 4}\n",
    "    \n",
    "    encoded_sequences = [\n",
    "        [base_to_index.get(nucleotide, 0) for nucleotide in sequence]  # Use 0 for padding and unknown bases\n",
    "        for sequence in data_frame[column_name]\n",
    "    ]\n",
    "\n",
    "    # Pad or truncate sequences to the desired length\n",
    "    for i in range(len(encoded_sequences)):\n",
    "        if len(encoded_sequences[i]) < sequence_length:\n",
    "            encoded_sequences[i] += [0] * (sequence_length - len(encoded_sequences[i]))\n",
    "        else:\n",
    "            encoded_sequences[i] = encoded_sequences[i][:sequence_length]\n",
    "\n",
    "    return np.array(encoded_sequences)\n",
    "\n",
    "data_set = 'all.csv'\n",
    "data = pd.read_csv(data_set)\n",
    "X_numerical = data[['skew', 'ratio']]\n",
    "sequence_length = 6\n",
    "X_dna_encoded = custom_dna_one_hot_encoder(data, 'Kmer', sequence_length)\n",
    "\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_numerical_poly = poly.fit_transform(X_numerical)\n",
    "\n",
    "# Data Normalization: Scale features to [0, 1] range\n",
    "scaler = StandardScaler()\n",
    "X_numerical_scaled = scaler.fit_transform(X_numerical_poly)\n",
    "print(X_numerical_scaled)\n",
    "\n",
    "X_final = np.concatenate([X_dna_encoded, X_numerical_scaled], axis=1)\n",
    "\n",
    "Y = data['c0'].values\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_final, Y, test_size=0.005, random_state=42)\n",
    "\n",
    "# Create the TensorFlow model using Keras\n",
    "def create_model(input_shape):\n",
    "    # DNA input branch\n",
    "    dna_inputs = tf.keras.layers.Input(shape=(6,))\n",
    "    x_dna = tf.keras.layers.Dense(512, activation='tanh', kernel_regularizer=tf.keras.regularizers.l2(.01))(dna_inputs)\n",
    "    x_dna = tf.keras.layers.Dropout(0.2)(x_dna)\n",
    "    x_dna = tf.keras.layers.Dense(256, activation='sigmoid', kernel_regularizer=tf.keras.regularizers.l2(.01))(x_dna)\n",
    "    x_dna = tf.keras.layers.Dropout(0.2)(x_dna)\n",
    "    x_dna = tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(.01))(x_dna)\n",
    "    x_dna = tf.keras.layers.Dropout(0.2)(x_dna)\n",
    "    \n",
    "    # Numerical input branch\n",
    "    numerical_inputs = tf.keras.layers.Input(shape=(5,))\n",
    "    x_num = tf.keras.layers.Dense(128, activation='relu')(numerical_inputs)\n",
    "    x_num = tf.keras.layers.Dense(64, activation = 'linear')(x_num)\n",
    "    \n",
    "    \n",
    "    # Merge branches\n",
    "    merged = tf.keras.layers.concatenate([x_dna, x_num])\n",
    "    \n",
    "    x = tf.keras.layers.Dense(4, activation='sigmoid')(merged)\n",
    "    x = tf.keras.layers.Dropout(0.2)(x)\n",
    "    outputs = tf.keras.layers.Dense(1, activation='linear')(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=[dna_inputs, numerical_inputs], outputs=outputs)\n",
    "    return model\n",
    "\n",
    "# Compile and train the model\n",
    "input_shape_dna = 6\n",
    "input_shape_num = 5\n",
    "model = create_model((input_shape_dna, input_shape_num))\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae', 'mse'])\n",
    "\n",
    "# Early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='mse', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "model.fit([X_train[:, :input_shape_dna], X_train[:, input_shape_dna:]], Y_train,\n",
    "          epochs=50, batch_size=64, callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate([X_test[:, :input_shape_dna], X_test[:, input_shape_dna:]], Y_test)\n",
    "print(f\"Test Loss: {loss}\")\n",
    "\n",
    "model_summary = get_model_layers_info(model)\n",
    "output_file = \"1_model_summary.csv\"\n",
    "write_model_summary_to_csv(model_summary, output_file, X_test, Y_test, data_set)\n",
    "\n",
    "\n",
    "# Save the model\n",
    "model.save('my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934adae0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DNcycP",
   "language": "python",
   "name": "dncycp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
